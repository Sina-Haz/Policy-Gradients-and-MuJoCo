Some hyper params to play with:
 - gamma (can reduce variance of our estimated policy gradient)
 - number of trajectories to collect per iteration
 - batch gradient descent (sample from trajectory) vs stochastic (one transition) vs whole trajectory
 - collision penalty (part of the reward function)
 - maybe train value function for a few iterations before training value + policy 
 - max Horizon per trajectory

Could try to use a fixed policy using heuristic and see if value function trains


1. Add obstacle position (relative or absolute) to observations
2. Make sparse reward higher (Combine)
3. Play around with nonlinear dense reward
4. Wandb to track training (loss, reward, etc.)
